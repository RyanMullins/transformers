#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/gemma3/modular_gemma3.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_gemma3.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
# coding=utf-8
# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.
#
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import itertools
import math
import re
from collections.abc import Sequence
from typing import Optional, Union, cast

import PIL.Image

from ...feature_extraction_utils import BatchFeature
from ...processing_utils import (
    ImagesKwargs,
    ProcessingKwargs,
    ProcessorMixin,
    TextKwargs,
    Unpack,
    _validate_images_text_input_order,
)
from ...tokenization_utils_base import AddedToken, TextInput
from ...utils import logging


logger = logging.get_logger(__name__)


class Gemma3TextKwargs(TextKwargs):
    num_mm_tokens_per_image: int


class Gemma3ImagesKwargs(ImagesKwargs):
    do_pan_and_scan: bool
    pan_and_scan_min_crop_size: int
    pan_and_scan_max_num_crops: int
    pan_and_scan_min_ratio_to_activate: float
    do_convert_rgb: Optional[bool]
    do_resize: bool
    size: dict[str, int]
    resample: PIL.Image.Resampling = (PIL.Image.Resampling.BICUBIC,)
    do_rescale: bool = (True,)
    rescale_factor: Union[int, float] = (1 / 255,)
    do_normalize: bool = (True,)
    image_mean: Optional[Union[float, list[float]]] = (None,)
    image_std: Optional[Union[float, list[float]]] = (None,)
    do_convert_rgb: bool = (None,)


class Gemma3ProcessorKwargs(ProcessingKwargs, total=False):
    text_kwargs: Gemma3TextKwargs
    images_kwargs: Gemma3ImagesKwargs
    _defaults = {
        "text_kwargs": {
            "num_mm_tokens_per_image": 256,
            "padding": False,
        },
        "images_kwargs": {
            "data_format": "channels_first",
            "do_pan_and_scan": False,
            "pan_and_scan_min_crop_size": 256,
            "pan_and_scan_max_num_crops": 4,
            "pan_and_scan_min_ratio_to_activate": 1.2,
        },
    }


IMAGE_PLACEHOLDER = "<image>"
IMAGE_PLACEHOLDER_LEN = len(IMAGE_PLACEHOLDER)
BEGIN_IMAGE_TOKEN = ""
END_IMAGE_TOKEN = ""
IMAGE_SOFT_TOKEN_PLACEHODLER = ""
NEWLINE_TOKEN = "\n"
PAN_AND_SCAN_PREFIX = "here is the original image"
PAN_AND_SCAN_POSTFIX = "and here are some crops to help you see better"
SPACE = " "

# Gemma 3 supports the following image input paradigms for any given prompt:
#
#   * No image      --> None
#   * Single-image  --> PIL.Image.Image
#   * Multi-image   --> Sequence[PIL.Image.Image]
#   * Batch         --> Sequence[Sequence[PIL.Image.Image]]
BatchedImageInput = Sequence[PIL.Image.Image]
BatchedMultiImageInput = Sequence[BatchedImageInput]
Gemma3ProcessorImageInput = Union[PIL.Image.Image, BatchedImageInput, BatchedMultiImageInput]

PanAndScannedImage = tuple[PIL.Image.Image, Sequence[PIL.Image.Image]]
BatchedPanAndScannedImage = Sequence[Sequence[PanAndScannedImage]]
MutablePanAndScannedImage = tuple[PIL.Image.Image, list[PIL.Image.Image]]
MutableBatchedPanAndScannedImage = list[list[MutablePanAndScannedImage]]

TextInputTypes = Union[TextInput, Sequence[TextInput]]


def pan_and_scan(
    image: PIL.Image.Image,
    pan_and_scan_min_crop_size: int,
    pan_and_scan_max_num_crops: int,
    pan_and_scan_min_ratio_to_activate: float,
    **unused_kwargs,
) -> Sequence[PIL.Image.Image]:
    w, h = image.size

    # Square or landscape image.
    if w >= h:
        # Only apply PaS if the image is sufficiently exaggerated
        if w / h < pan_and_scan_min_ratio_to_activate:
            return []

        # Select ideal number of crops close to the image aspect ratio and such that crop_size > min_crop_size.
        num_crops_w = int(math.floor(w / h + 0.5))  # Half round up rounding.
        num_crops_w = min(int(math.floor(w / pan_and_scan_min_crop_size)), num_crops_w)

        # Make sure the number of crops is in range [2, pan_and_scan_max_num_crops].
        num_crops_w = max(2, num_crops_w)
        num_crops_w = min(pan_and_scan_max_num_crops, num_crops_w)
        num_crops_h = 1

    # Portrait image.
    else:
        # Only apply PaS if the image is sufficiently exaggerated
        if h / w < pan_and_scan_min_ratio_to_activate:
            return []

        # Select ideal number of crops close to the image aspect ratio and such that crop_size > min_crop_size.
        num_crops_h = int(math.floor(h / w + 0.5))
        num_crops_h = min(int(math.floor(h / pan_and_scan_min_crop_size)), num_crops_h)

        # Make sure the number of crops is in range [2, pan_and_scan_max_num_crops].
        num_crops_h = max(2, num_crops_h)
        num_crops_h = min(pan_and_scan_max_num_crops, num_crops_h)
        num_crops_w = 1

    crop_size_w = int(math.ceil(w / num_crops_w))
    crop_size_h = int(math.ceil(h / num_crops_h))

    # Don't apply PaS if crop size is too small.
    if min(crop_size_w, crop_size_h) < pan_and_scan_min_crop_size:
        return []

    crop_positions_w = [crop_size_w * i for i in range(num_crops_w)]
    crop_positions_h = [crop_size_h * i for i in range(num_crops_h)]

    # Generate crops.
    return [
        image.crop((pos_w, pos_h, pos_w + crop_size_w, pos_h + crop_size_h))
        for pos_h, pos_w in itertools.product(crop_positions_h, crop_positions_w)
    ]


class Gemma3Processor(ProcessorMixin):
    attributes = ["image_processor", "tokenizer"]
    valid_kwargs = ["chat_template"]
    image_processor_class = "SiglipImageProcessor"
    tokenizer_class = ("GemmaTokenizer", "GemmaTokenizerFast")

    def __init__(self, image_processor=None, tokenizer=None, chat_template=None, **kwargs):
        if image_processor is None:
            raise ValueError("You need to specify an `image_processor`.")
        if tokenizer is None:
            raise ValueError("You need to specify a `tokenizer`.")

        try:
            self.image_seq_length = getattr(image_processor, "image_seq_length")
        except AttributeError as e:
            raise ValueError("`image_processor` is missing the required `image_seq_length` attribute.") from e

        try:
            self.image_token_id = getattr(tokenizer, "image_token_id")
        except AttributeError:
            logger.warning("Image token not provided by `tokenizer`. Adding special `<image>` token.")

            image_token = AddedToken(IMAGE_PLACEHOLDER, normalized=False, special=True)
            tokens_to_add = {"additional_special_tokens": [image_token]}
            tokenizer.add_special_tokens(tokens_to_add)
            self.image_token_id = tokenizer.convert_tokens_to_ids(IMAGE_PLACEHOLDER)

        self.image_token = tokenizer.decode(self.image_token_id)

        super().__init__(
            image_processor=image_processor,
            tokenizer=tokenizer,
            chat_template=chat_template,
            **kwargs,
        )

    def __call__(
        self,
        images: Optional[Gemma3ProcessorImageInput] = None,
        text: Optional[TextInputTypes] = None,
        **kwargs: Unpack[Gemma3ProcessorKwargs],
    ) -> BatchFeature:
        if text is None and images is None:
            raise ValueError("Provide at least one of `text` or `images`.")

        # Check if images and text inputs are reversed for backward compatibility
        images, text = _validate_images_text_input_order(images, text)

        output_kwargs = self._merge_kwargs(
            Gemma3ProcessorKwargs,
            tokenizer_init_kwargs=self.tokenizer.init_kwargs,
            **kwargs,
        )

        if images is not None:
            batched_images = self._process_images(images=images, **output_kwargs["images_kwargs"])
            flattened_images = self._make_flat_list_of_images(batched_images=batched_images)
            pixel_values = self.image_processor(flattened_images, **output_kwargs["images_kwargs"])["pixel_values"]
        else:
            batched_images = None
            pixel_values = None

        batched_input = self._process_text(text=text, batched_images=batched_images, **output_kwargs["text_kwargs"])

        if pixel_values is not None:
            batched_input.update(pixel_values=pixel_values)

        return batched_input

    def _process_images(
        self, images: Gemma3ProcessorImageInput, **kwargs: Unpack[Gemma3ImagesKwargs]
    ) -> BatchedPanAndScannedImage:
        # Normalize image structures
        if isinstance(images, PIL.Image.Image):
            images_lists: MutableBatchedPanAndScannedImage = [[(images, [])]]
        elif isinstance(images[0], PIL.Image.Image):
            images = cast(BatchedImageInput, images)
            images_lists: MutableBatchedPanAndScannedImage = [[(i, [])] for i in images]
        else:
            images = cast(BatchedMultiImageInput, images)
            images_lists: MutableBatchedPanAndScannedImage = [[(i, []) for i in il] for il in images]

        # if not all(len(images_lists[0]) == len(l) for l in images_lists):
        #     raise ValueError("All elements in a batch must have the same number of images.")

        if kwargs["do_pan_and_scan"]:
            if not isinstance(images_lists[0][0], PIL.Image.Image):
                raise ValueError("Pan and scan is only supported for `Pillow.Image.Image` inputs")

            for images_list in images_lists:
                for image, crops in images_list:
                    crops.extend(pan_and_scan(image=image, **kwargs))

        return images_lists

    def _process_text(
        self,
        text: Optional[TextInputTypes] = None,
        batched_images: Optional[BatchedPanAndScannedImage] = None,
        **kwargs: Unpack[Gemma3TextKwargs],
    ) -> BatchFeature:
        if batched_images and not text:
            text = [" ".join([IMAGE_PLACEHOLDER] * len(images)) for images in batched_images]

        if batched_images and text:
            if isinstance(text, str):
                text = [text]

            if (bi_l := len(batched_images)) != (t_l := len(text)):
                raise ValueError(f"Received inconsistently sized batches of images ({bi_l}) and text ({t_l}).")

            num_mm_tokens_per_image = kwargs["num_mm_tokens_per_image"]
            image_string_for_tokenization = (
                NEWLINE_TOKEN
                + BEGIN_IMAGE_TOKEN
                + "".join([IMAGE_SOFT_TOKEN_PLACEHODLER] * num_mm_tokens_per_image)
                + END_IMAGE_TOKEN
                + NEWLINE_TOKEN
            )

            for prompt, images in zip(text, batched_images):
                image_indexes = [m.start() for m in re.finditer(IMAGE_PLACEHOLDER, prompt)]
                if (i_l := len(images)) != (iidx_l := len(image_indexes)):
                    raise ValueError(f"Prompt contained {iidx_l} image placeholders but received {i_l} images.")

                for (image, pas_images), idx in reversed(zip(images, image_indexes)):
                    if pas_images:
                        formatted_image_text = SPACE.join(
                            [
                                PAN_AND_SCAN_PREFIX,
                                image_string_for_tokenization,
                                PAN_AND_SCAN_POSTFIX,
                            ]
                            + [image_string_for_tokenization] * len(pas_images)
                        )
                    else:
                        formatted_image_text = image_string_for_tokenization

                    prompt = prompt[:idx] + formatted_image_text + prompt[idx + IMAGE_PLACEHOLDER_LEN :]

        inputs = self.tokenizer(text=text, **kwargs)
        return BatchFeature(data={**inputs})

    def _make_flat_list_of_images(
        self,
        batched_images: BatchedPanAndScannedImage,
    ) -> Sequence[PIL.Image.Image]:
        flattened_images: list[PIL.Image.Image] = []

        for images in batched_images:
            for image, pas_images in images:
                flattened_images.extend([image] + pas_images)

        return flattened_images
